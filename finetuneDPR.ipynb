{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Finetuning DPR(Dense Passage Retriever) model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zZghGYoILtsK",
    "outputId": "581a1132-658c-4b30-9848-f78e9c3bdb4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tOmlSv113k3D",
    "outputId": "0008db15-8718-4d33-8258-145ae0818949"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia-api\n",
      "  Downloading wikipedia_api-0.7.1.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from wikipedia-api) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2024.8.30)\n",
      "Building wheels for collected packages: wikipedia-api\n",
      "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.7.1-py3-none-any.whl size=14346 sha256=2f31a5094432cbe8a9bbd82298e23c19db5d83b20a036c0e1807da96137ba45e\n",
      "  Stored in directory: /root/.cache/pip/wheels/4c/96/18/b9201cc3e8b47b02b510460210cfd832ccf10c0c4dd0522962\n",
      "Successfully built wikipedia-api\n",
      "Installing collected packages: wikipedia-api\n",
      "Successfully installed wikipedia-api-0.7.1\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia-api\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6EykhHi44OKE",
    "outputId": "0f1c7644-04b2-47be-b584-cc8faa85de51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QohoyT7Z3th2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wikipediaapi\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import Dataset\n",
    "from transformers import get_scheduler\n",
    "import re\n",
    "from transformers import (DPRContextEncoderTokenizer, DPRQuestionEncoderTokenizer, DPRQuestionEncoder, DPRContextEncoder, DPRReader, Trainer, TrainingArguments)\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Px41cbcHcsXR",
    "outputId": "fcb75a2a-9804-4cdd-ba9a-bdcbc05f8812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"GPU not available. Check runtime settings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of pages related to the given plant name from wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "50G4l6lU4AQ5"
   },
   "outputs": [],
   "source": [
    "wiki = wikipediaapi.Wikipedia('plantInfoRetrieval(haricharangoudca1@gmail.com)', 'en' )\n",
    "\n",
    "def fetch_page(plant_name):\n",
    "  page = wiki.page(plant_name)\n",
    "  if page.exists:\n",
    "    return page.text\n",
    "  else:\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Hmc2VvE5N6t",
    "outputId": "3505fbe5-9479-4195-d4bc-95eaf4857c32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A rose is either a woody perennial flowering plant of the genus Rosa (), in the family Rosaceae (), or the flower it bears. There are over three hundred species and tens of thousands of cultivars. The\n"
     ]
    }
   ],
   "source": [
    "plant = 'rose'\n",
    "plant_info = fetch_page(plant)\n",
    "if plant_info:\n",
    "  print(plant_info[:200])\n",
    "else:\n",
    "  print(f\"No information found for {plant}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lYc_XjeDgBRt"
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df['query'] = df['Plant Name']\n",
    "    df['context'] = (\n",
    "        \"Plant Family: \" + df['Family'] + \"\\n\"\n",
    "        + \"Description: \" + df['Description'] + \"\\n\"\n",
    "        + \"Uses: \" + df['Uses']\n",
    "    )\n",
    "    return df[['query', 'context']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "vGcb1gBEf9iq"
   },
   "outputs": [],
   "source": [
    "def initialize_models_and_tokenizers(device):\n",
    "    query_model = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base').to(device)\n",
    "    passage_model = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base').to(device)\n",
    "\n",
    "    query_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "    passage_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "\n",
    "    return query_model, passage_model, query_tokenizer, passage_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "iYdYHhWuf9f2"
   },
   "outputs": [],
   "source": [
    "def create_dense_layer(input_size, output_size):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_size, output_size * 2),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(output_size * 2, output_size),\n",
    "        nn.GELU(),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "jQ2l_XDPf9dR"
   },
   "outputs": [],
   "source": [
    "def batch_tokenize(df, query_tokenizer, passage_tokenizer, batch_size=2, sample_size=4):\n",
    "    rand_idx = np.random.randint(0, len(df), (batch_size, sample_size))\n",
    "    queries = []\n",
    "    contexts = []\n",
    "    true_idx = []\n",
    "\n",
    "    for row_idx, row in enumerate(rand_idx):\n",
    "        rand_query_idx = random.randint(0, len(df) - 1)\n",
    "        query = df.iloc[rand_query_idx]['query']\n",
    "        true_context = df.iloc[rand_query_idx]['context']\n",
    "        queries.append(query)\n",
    "\n",
    "        if rand_query_idx not in row:\n",
    "            idx = random.randint(0, sample_size - 1)\n",
    "            rand_idx[row_idx][idx] = rand_query_idx\n",
    "            true_idx.append(idx)\n",
    "        else:\n",
    "            true_idx.append(np.where(rand_idx[row_idx] == rand_query_idx)[0][0])\n",
    "\n",
    "        for col_idx in row:\n",
    "            context = df.iloc[col_idx]['context']\n",
    "            contexts.append(context)\n",
    "\n",
    "    passage_tensor = passage_tokenizer(contexts, padding='longest', return_tensors=\"pt\")\n",
    "    query_tensor = query_tokenizer(queries, padding='longest', return_tensors=\"pt\")\n",
    "\n",
    "    return passage_tensor, query_tensor, true_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kmc3GhdEf9aO"
   },
   "outputs": [],
   "source": [
    "def dot_product_similarity(query_embeddings, passage_embeddings):\n",
    "    query_embeddings = query_embeddings.unsqueeze(1)\n",
    "    similarity_scores = torch.matmul(query_embeddings, passage_embeddings.transpose(-2, -1))\n",
    "    return similarity_scores\n",
    "def forward_pass(\n",
    "    query_model, passage_model, query_tokenizer, passage_tokenizer,\n",
    "    df, passage_dense_layer, query_dense_layer,\n",
    "    device, batch_size=2, sample_size=4\n",
    "):\n",
    "    passage_tensor, query_tensor, true_idx = batch_tokenize(df, query_tokenizer, passage_tokenizer, batch_size, sample_size)\n",
    "\n",
    "    passage_tensor = {key: val.to(device) for key, val in passage_tensor.items()}\n",
    "    query_tensor = {key: val.to(device) for key, val in query_tensor.items()}\n",
    "\n",
    "    dense_passage = passage_model(input_ids=passage_tensor['input_ids'], attention_mask=passage_tensor['attention_mask'])\n",
    "    dense_query = query_model(input_ids=query_tensor['input_ids'], attention_mask=query_tensor['attention_mask'])\n",
    "\n",
    "    dense_passage = dense_passage['pooler_output']\n",
    "    dense_passage = dense_passage.reshape(batch_size, sample_size, -1).to(device)\n",
    "    dense_query = dense_query['pooler_output'].to(device)\n",
    "\n",
    "    dense_passage = passage_dense_layer(dense_passage)\n",
    "    dense_query = query_dense_layer(dense_query)\n",
    "\n",
    "    similarity_scores = dot_product_similarity(dense_query, dense_passage)\n",
    "    similarity_scores = similarity_scores.squeeze(1)\n",
    "\n",
    "    log_scores = F.log_softmax(similarity_scores, dim=1)\n",
    "\n",
    "    return log_scores, true_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "f1aJsbqIf9XL"
   },
   "outputs": [],
   "source": [
    "def compute_loss(log_scores, true_idx,device):\n",
    "    true_idx = torch.tensor(true_idx, dtype=torch.long, device = device)\n",
    "    loss = F.nll_loss(log_scores, true_idx)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "T-wOk25hf9Un"
   },
   "outputs": [],
   "source": [
    "def finetune_model(\n",
    "    query_model, passage_model, query_tokenizer, passage_tokenizer,\n",
    "    df, passage_dense_layer, query_dense_layer,\n",
    "    device, epochs, batch_size, sample_size, learning_rate,\n",
    "    model_save_path=\"/content/drive/MyDrive/fine_tuned_model\"\n",
    "):\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        list(query_model.parameters())\n",
    "        + list(passage_model.parameters())\n",
    "        + list(passage_dense_layer.parameters())\n",
    "        + list(query_dense_layer.parameters()),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "\n",
    "    total_steps = (len(df) // batch_size) * epochs\n",
    "    scheduler = get_scheduler(\n",
    "        \"linear\", optimizer=optimizer, num_warmup_steps=int(total_steps * 0.1), num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        query_model.train()\n",
    "        passage_model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for _ in range(len(df) // batch_size):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            log_scores, true_idx = forward_pass(\n",
    "                query_model, passage_model, query_tokenizer, passage_tokenizer,\n",
    "                df, passage_dense_layer, query_dense_layer,\n",
    "                device, batch_size, sample_size\n",
    "            )\n",
    "\n",
    "            loss = compute_loss(log_scores, true_idx, device)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(query_model.parameters(), max_norm=1.0)\n",
    "            torch.nn.utils.clip_grad_norm_(passage_model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    print(\"Saving fine-tuned models...\")\n",
    "    query_model.save_pretrained(f\"{model_save_path}/query_model\")\n",
    "    passage_model.save_pretrained(f\"{model_save_path}/passage_model\")\n",
    "    torch.save(passage_dense_layer.state_dict(), f\"{model_save_path}/passage_dense_layer.pth\")\n",
    "    torch.save(query_dense_layer.state_dict(), f\"{model_save_path}/query_dense_layer.pth\")\n",
    "    print(f\"Models saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PGPmbROAf9R3",
    "outputId": "c2b320eb-9e82-4a43-acc2-28f89b26bd47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training Loss: 2.0801\n",
      "Epoch 2/10, Training Loss: 2.0739\n",
      "Epoch 3/10, Training Loss: 2.0691\n",
      "Epoch 4/10, Training Loss: 2.2256\n",
      "Epoch 5/10, Training Loss: 2.0834\n",
      "Epoch 6/10, Training Loss: 2.2512\n",
      "Epoch 7/10, Training Loss: 2.0881\n",
      "Epoch 8/10, Training Loss: 2.2328\n",
      "Epoch 9/10, Training Loss: 2.0728\n",
      "Epoch 10/10, Training Loss: 2.1214\n",
      "Saving fine-tuned models...\n",
      "Models saved to /content/drive/MyDrive/fine_tuned_model\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    csv_path = '/content/drive/MyDrive/plants_dataset.csv'\n",
    "    df = load_and_preprocess_data(csv_path)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    query_model, passage_model, query_tokenizer, passage_tokenizer = initialize_models_and_tokenizers(device)\n",
    "\n",
    "    dense_size = 128\n",
    "    passage_dense_layer = create_dense_layer(768, dense_size).to(device)\n",
    "    query_dense_layer = create_dense_layer(768, dense_size).to(device)\n",
    "\n",
    "    model_save_path = \"/content/drive/MyDrive/fine_tuned_model\"\n",
    "    finetune_model(\n",
    "        query_model, passage_model, query_tokenizer, passage_tokenizer,\n",
    "        df, passage_dense_layer, query_dense_layer,\n",
    "        device, epochs=10, batch_size=10, sample_size=8, learning_rate=5e-3,\n",
    "        model_save_path=model_save_path\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gbwsk7p6k8SA",
    "outputId": "b6ee25a3-4632-4569-ac80-03f02b675130"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query: clove\n",
      "\n",
      "Relevant Information:\n",
      "\n",
      "    Family: None\n",
      "    Description: and are commonly used as a spice, flavoring, or fragrance in consumer products, such as toothpaste, soaps, or cosmetics. ...\n",
      "    Uses: Cloves are used in the cuisine of Asian, African, Mediterranean, and the Near and Middle East countries, lending flavor to meats (such as baked ham), curries, and marinades, as well as fruit (such as apples, pears, and rhubarb) ...\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "\n",
    "def extract_relevant_info(text):\n",
    "    family_name = None\n",
    "    description = None\n",
    "    uses = None\n",
    "\n",
    "    family_pattern = re.search(\n",
    "        r\"(?i)((\\b(plant\\s*family|family\\s*name|genus)\\s*[:\\-]?\\s*[\\w\\s]+)|\" +\n",
    "        r\"belongs\\s+to\\s+the\\s+(family|genus)\\b.*?\\.\" +\n",
    "        r\"|is\\s+a\\s+member\\s+of\\s+the\\s+(family|genus)\\b.*?\\.\" +\n",
    "        r\"|classified\\s+under\\s+(the\\s+family|the\\s+genus)\\b.*?\\.\" +\n",
    "        r\"|is\\s+from\\s+the\\s+(family|genus)\\b.*?\\.\" +\n",
    "        r\"|related\\s+to\\s+(the\\s+family|the\\s+genus)\\b.*?\\.)\", text)\n",
    "\n",
    "    description_pattern = re.search(\n",
    "        r\"(?i)(\\b\\w+\\s+(is|are)\\s+(a|an|widely|commonly|known|characterized|used|identified)\\b.*?\\.\" +\n",
    "        r\"|belongs\\s+to\\b.*?\\.\" +\n",
    "        r\"|has\\s+(properties|features|characteristics)\\b.*?\\.\" +\n",
    "        r\"|can\\s+be\\s+(identified|used|found)\\b.*?\\.)\", text)\n",
    "\n",
    "    uses_pattern = re.search(r\"(?i)(uses\\s*[:\\-]?\\s*([^.]+))\", text)\n",
    "\n",
    "    if family_pattern:\n",
    "        family_name = family_pattern.group(0).strip()\n",
    "\n",
    "    if description_pattern:\n",
    "        description = description_pattern.group(0).strip()\n",
    "\n",
    "    if uses_pattern:\n",
    "        uses = uses_pattern.group(2).strip()\n",
    "\n",
    "    if description:\n",
    "        description = limit_text_length(description, min_length=50, max_length=100)\n",
    "\n",
    "    if uses:\n",
    "        uses = limit_text_length(uses, min_length=50, max_length=100)\n",
    "\n",
    "    if not description:\n",
    "        description = extract_first_sentences(text, num_sentences=2)\n",
    "\n",
    "    return {\n",
    "        \"family_name\": family_name,\n",
    "        \"description\": description,\n",
    "        \"uses\": uses\n",
    "    }\n",
    "\n",
    "def limit_text_length(text, min_length=50, max_length=100):\n",
    "    words = text.split()\n",
    "    if len(words) > max_length:\n",
    "        text = \" \".join(words[:max_length]) + \"...\"\n",
    "    elif len(words) < min_length:\n",
    "        text = \" \".join(words + ['...'])\n",
    "    return text\n",
    "\n",
    "def extract_first_sentences(text, num_sentences=2):\n",
    "    sentences = text.split(\". \")\n",
    "    return \". \".join(sentences[:num_sentences]) + (\"\" if text.endswith(\".\") else \".\")\n",
    "\n",
    "def retrieve_relevant_documents(query):\n",
    "    document_text = fetch_page(query)\n",
    "\n",
    "    if not document_text:\n",
    "        return [\"Sorry, no relevant information found.\"]\n",
    "\n",
    "    return extract_relevant_info(document_text)\n",
    "\n",
    "def predict(user_input):\n",
    "    query_tensor = query_tokenizer(user_input, padding='longest', truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        dense_query = query_model(**query_tensor)['pooler_output']\n",
    "    dense_query = query_dense_layer(dense_query)\n",
    "\n",
    "    relevant_info = retrieve_relevant_documents(user_input)\n",
    "\n",
    "    document_list = [f\"\"\"\n",
    "    Family: {relevant_info.get('family_name', 'Not available')}\n",
    "    Description: {relevant_info.get('description', 'Not available')}\n",
    "    Uses: {relevant_info.get('uses', 'Not available')}\n",
    "    \"\"\"]\n",
    "\n",
    "    passage_tensor = passage_tokenizer(document_list, padding='longest', truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        dense_passage = passage_model(input_ids=passage_tensor['input_ids'], attention_mask=passage_tensor['attention_mask'])['pooler_output']\n",
    "    dense_passage = passage_dense_layer(dense_passage)\n",
    "\n",
    "    similarity_scores = torch.matmul(dense_query, dense_passage.transpose(-2, -1)).squeeze()\n",
    "    most_similar_index = torch.argmax(similarity_scores).item()\n",
    "\n",
    "    return document_list[most_similar_index]\n",
    "\n",
    "user_input = input(\"Enter your query: \")\n",
    "output = predict(user_input)\n",
    "print(f\"\\nRelevant Information:\\n{output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aTwRWNGwnMCV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
